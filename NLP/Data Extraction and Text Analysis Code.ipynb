{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbe871f",
   "metadata": {},
   "source": [
    "# <center> Data Extraction and Text Analysis <center>  Blackcoffer Consulting <center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a03145",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objective \n",
    "Objective of this assignment is to extract some sections (which are mentioned below) from SEC / EDGAR financial reports and perform text analysis to compute variables those are explained below\n",
    "<ul>\n",
    "<li>Section 1.1: Positive score, negative score, polarity score</li>\n",
    "<li>Section 2: Average Sentence Length, percentage of complex words, fog index</li>\n",
    "<li>Section 4: Complex word count</li>\n",
    "<li>Section 5: Word count</li>\n",
    "<li>Uncertainty and Constraining</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d6bbe",
   "metadata": {},
   "source": [
    "## Reading in the Data using Pandas From Excel File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c303442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requried imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb23f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Text extraction patterns #########################\n",
    "\n",
    "mda_regex = (r\"item[^a-zA-Z\\n]*\\d\\s*\\.\\s*management\\'s discussion and analysis.*?^\\s*item[^a-zA-Z\\n]*\\d\\s*\\.*\")\n",
    "\n",
    "qqd_regex = (r\"item[^a-zA-Z\\n]*\\d[a-z]?\\.?\\s*Quantitative and Qualitative Disclosures about \" \\\n",
    "            r\"Market Risk.*?^\\s*item\\s*\\d\\s*\")\n",
    "\n",
    "riskfactor_regex = (r\"item[^a-zA-Z\\n]*\\d[a-z]?\\.?\\s*Risk Factors.*?^\\s*item\\s*\\d\\s*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7d6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Filepath locations #########################\n",
    "stopWordsFile = 'C:\\\\Users\\\\HP\\\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\StopWords_Generic.txt'\n",
    "positiveWordsFile = 'C:\\\\Users\\\\HP\\\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\PositiveWord.txt'\n",
    "nagitiveWordsFile = 'C:\\\\Users\\\\HP\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\NegativeWord.txt'\n",
    "uncertainty_dictionaryFile = 'C:\\\\Users\\\\HP\\\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\uncertainty_dictionary.txt'\n",
    "constraining_dictionaryFile = 'C:\\\\Users\\\\HP\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\constraining_dictionary.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc539012",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Function for extracting requried text #########################\n",
    "def rawdata_extract(path, cikListFile):\n",
    "    html_regex = re.compile(r'<.*?>')\n",
    "    extraxted_data=[]\n",
    "    \n",
    "    \n",
    "    cikListFile = pd.read_csv(cikListFile)\n",
    "    for index, row in cikListFile.iterrows():\n",
    "        processingFile=row['SECFNAME'].split(\"/\")\n",
    "        inputFile = processingFile[3]\n",
    "        cik=row['CIK']\n",
    "        coname=row['CONAME']\n",
    "        fyrmo=row['FYRMO']\n",
    "        fdate = row['FDATE']\n",
    "        form = row['FORM']\n",
    "        secfname=row['SECFNAME']\n",
    "        for fileName in os.listdir(path):\n",
    "            filenameopen = os.path.join(path, fileName)\n",
    "            dirFileName = filenameopen.split('\\\\')\n",
    "            currentFile=dirFileName[1]\n",
    "\n",
    "            if os.path.isfile(filenameopen) and currentFile == inputFile :\n",
    "                resultdict = dict()\n",
    "                resultdict['CIK'] = cik\n",
    "                resultdict['CONAME'] = coname\n",
    "                resultdict['FYRMO'] = fyrmo\n",
    "                resultdict['FDATE'] = fdate\n",
    "                resultdict['FORM'] = form\n",
    "                resultdict['SECFNAME'] = secfname\n",
    "                \n",
    "                with open(filenameopen, 'r', encoding='utf-8', errors=\"replace\") as in_file:\n",
    "                    content = in_file.read()\n",
    "                    content = re.sub(html_regex,'',content)\n",
    "                    content = content.replace('&nbsp;','')\n",
    "                    content = re.sub(r'&#\\d+;', '', content)\n",
    "                    matches_mda = re.findall(mda_regex, content, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "                    if matches_mda:\n",
    "                        result = max(matches_mda, key=len)\n",
    "                        result = str(result).replace('\\n', '')\n",
    "                        resultdict['mda_extract'] = result\n",
    "                    else:\n",
    "                        resultdict['mda_extract'] = \"\"\n",
    "                    match_qqd = re.findall(qqd_regex, content, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "                    if match_qqd:\n",
    "                        result_qqd = max(match_qqd, key=len)\n",
    "                        result_qqd = str(result_qqd).replace('\\n','')\n",
    "                        resultdict['qqd_extract']= result_qqd\n",
    "                    else:\n",
    "                        resultdict['qqd_extract'] = \"\"\n",
    "                    match_riskfactor = re.findall(riskfactor_regex, content, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "                    if match_riskfactor:\n",
    "                        result_riskfactor = max(match_riskfactor, key=len)\n",
    "                        result_riskfactor = str(result_riskfactor).replace('\\n', '')\n",
    "                        resultdict['riskfactor_extract'] = result_riskfactor\n",
    "                    else:\n",
    "                        resultdict['riskfactor_extract'] = \"\"\n",
    "                    extraxted_data.append(resultdict)\n",
    "\n",
    "                in_file.close()\n",
    "\n",
    "    return extraxted_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538acee2",
   "metadata": {},
   "source": [
    "## Section 1.1: Positive score, negative score, polarity score\n",
    "#### Loading stop words dictionary for removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66438636",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopWordsFile ,'r') as stop_words:\n",
    "    stopWords = stop_words.read().lower()\n",
    "stopWordList = stopWords.split('\\n')\n",
    "stopWordList[-1:] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb623e",
   "metadata": {},
   "source": [
    "#### Tokenizeing module and filtering tokens using stop words list, removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b494a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Tokenizer #########################\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_words = list(filter(lambda token: token not in stopWordList, tokens))\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6ec9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Loading positive words #########################\n",
    "with open(positiveWordsFile,'r') as posfile:\n",
    "    positivewords=posfile.read().lower()\n",
    "positiveWordList=positivewords.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3491ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Loading positive words #########################\n",
    "with open(positiveWordsFile,'r') as posfile:\n",
    "    positivewords=posfile.read().lower()\n",
    "positiveWordList=positivewords.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43137c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Calculating positive score  #########################\n",
    "def positive_score(text):\n",
    "    numPosWords = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in positiveWordList:\n",
    "            numPosWords  += 1\n",
    "    \n",
    "    sumPos = numPosWords\n",
    "    return sumPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b660fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Calculating Negative score #########################\n",
    "def negative_word(text):\n",
    "    numNegWords=0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in negativeWordList:\n",
    "            numNegWords -=1\n",
    "    sumNeg = numNegWords \n",
    "    sumNeg = sumNeg * -1\n",
    "    return sumNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db94f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Calculating polarity score #########################\n",
    "def polarity_score(positiveScore, negativeScore):\n",
    "    pol_score = (positiveScore - negativeScore) / ((positiveScore + negativeScore) + 0.000001)\n",
    "    return pol_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757599eb",
   "metadata": {},
   "source": [
    "## Section 2 -Analysis of Readability - Average Sentence Length, percentage of complex words, fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1285529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Average sentence length \n",
    "# It will calculated using formula --- Average Sentence Length = the number of words / the number of sentences\n",
    "     \n",
    "def average_sentence_length(text):\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    tokens = tokenizer(text)\n",
    "    totalWordCount = len(tokens)\n",
    "    totalSentences = len(sentence_list)\n",
    "    average_sent = 0\n",
    "    if totalSentences != 0:\n",
    "        average_sent = totalWordCount / totalSentences\n",
    "    \n",
    "    average_sent_length= average_sent\n",
    "    \n",
    "    return round(average_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec9d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage of complex word \n",
    "# It is calculated using Percentage of Complex words = the number of complex words / the number of words \n",
    "\n",
    "def percentage_complex_word(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    complex_word_percentage = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(tokens) != 0:\n",
    "        complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "    return complex_word_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8212e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Fog Index \n",
    "# Fog index is calculated using -- Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "    return fogIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460ab3b",
   "metadata": {},
   "source": [
    "## Section 4: Complex word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3abb979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting complex words\n",
    "def complex_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    return complexWord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e5fe6",
   "metadata": {},
   "source": [
    "## Section 5: Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4106b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting total words\n",
    "\n",
    "def total_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9c9f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating uncertainty_score\n",
    "with open(uncertainty_dictionaryFile ,'r' ) as uncertain_dict:\n",
    "    uncertainDict=uncertain_dict.read().lower()\n",
    "uncertainDictionary = uncertainDict.split('\\n')\n",
    "\n",
    "def uncertainty_score(text):\n",
    "    uncertainWordnum =0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in uncertainDictionary:\n",
    "            uncertainWordnum +=1\n",
    "    sumUncertainityScore = uncertainWordnum \n",
    "    \n",
    "    return sumUncertainityScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e0e5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating constraining score\n",
    "with open(constraining_dictionaryFile , 'r') as constraining_dict:\n",
    "    constrainDict=constraining_dict.read().lower()\n",
    "constrainDictionary = constrainDict.split('\\n')\n",
    "\n",
    "def constraining_score(text):\n",
    "    constrainWordnum =0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in constrainDictionary:\n",
    "            constrainWordnum +=1\n",
    "    sumConstrainScore = constrainWordnum \n",
    "    \n",
    "    return sumConstrainScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78d6d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating positive word proportion\n",
    "\n",
    "def positive_word_prop(positiveScore,wordcount):\n",
    "    positive_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        positive_word_proportion = positiveScore / wordcount\n",
    "        \n",
    "    return positive_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8b760ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating negative word proportion\n",
    "\n",
    "def negative_word_prop(negativeScore,wordcount):\n",
    "    negative_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        negative_word_proportion = negativeScore / wordcount\n",
    "        \n",
    "    return negative_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c97b6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating uncertain word proportion\n",
    "\n",
    "def uncertain_word_prop(uncertainScore,wordcount):\n",
    "    uncertain_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        uncertain_word_proportion = uncertainScore / wordcount\n",
    "        \n",
    "    return uncertain_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4e5c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating constraining word proportion\n",
    "\n",
    "def constraining_word_prop(constrainingScore,wordcount):\n",
    "    constraining_word_proportion = 0\n",
    "    if wordcount !=0:\n",
    "        constraining_word_proportion = constrainingScore / wordcount\n",
    "        \n",
    "    return constraining_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3f8edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Constraining words for whole report\n",
    "\n",
    "def constrain_word_whole(mdaText,qqdmrText,rfText):\n",
    "    wholeDoc = mdaText + qqdmrText + rfText\n",
    "    constrainWordnumWhole =0\n",
    "    rawToken = tokenizer(wholeDoc)\n",
    "    for word in rawToken:\n",
    "        if word in constrainDictionary:\n",
    "            constrainWordnumWhole +=1\n",
    "    sumConstrainScoreWhole = constrainWordnumWhole \n",
    "    \n",
    "    return sumConstrainScoreWhole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18cbb59a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6eb9f1520b1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minputDirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\HP\\\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\test'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmasterFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\HP\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\cik_list.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrawdata_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputDirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasterFile\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9911cb4b6706>\u001b[0m in \u001b[0;36mrawdata_extract\u001b[1;34m(path, cikListFile)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcikListFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprocessingFile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SECFNAME'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0minputFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessingFile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mcik\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CIK'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mconame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CONAME'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "inputDirectory = 'C:\\\\Users\\\\HP\\\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\test'\n",
    "masterFile = 'C:\\\\Users\\\\HP\\Desktop\\\\INTERNSHIP ASSIGNMENT\\\\cik_list.csv'\n",
    "dataList = rawdata_extract(inputDirectory, masterFile )\n",
    "df = pd.DataFrame(dataList)\n",
    "\n",
    "df['mda_positive_score'] = df.mda_extract.apply(positive_score)\n",
    "df['mda_negative_score'] = df.mda_extract.apply(negative_word)\n",
    "df['mda_polarity_score'] = np.vectorize(polarity_score)(df['mda_positive_score'],df['mda_negative_score'])\n",
    "df['mda_average_sentence_length'] = df.mda_extract.apply(average_sentence_length)\n",
    "df['mda_percentage_of_complex_words'] = df.mda_extract.apply(percentage_complex_word)\n",
    "df['mda_fog_index'] = np.vectorize(fog_index)(df['mda_average_sentence_length'],df['mda_percentage_of_complex_words'])\n",
    "df['mda_complex_word_count']= df.mda_extract.apply(complex_word_count)\n",
    "df['mda_word_count'] = df.mda_extract.apply(total_word_count)\n",
    "df['mda_uncertainty_score']=df.mda_extract.apply(uncertainty_score)\n",
    "df['mda_constraining_score'] = df.mda_extract.apply(constraining_score)\n",
    "df['mda_positive_word_proportion'] = np.vectorize(positive_word_prop)(df['mda_positive_score'],df['mda_word_count'])\n",
    "df['mda_negative_word_proportion'] = np.vectorize(negative_word_prop)(df['mda_negative_score'],df['mda_word_count'])\n",
    "df['mda_uncertainty_word_proportion'] = np.vectorize(uncertain_word_prop)(df['mda_uncertainty_score'],df['mda_word_count'])\n",
    "df['mda_constraining_word_proportion'] = np.vectorize(constraining_word_prop)(df['mda_constraining_score'],df['mda_word_count'])\n",
    "\n",
    "df['qqdmr_positive_score'] = df.qqd_extract.apply(positive_score)\n",
    "df['qqdmr_negative_score'] = df.qqd_extract.apply(negative_word)\n",
    "df['qqdmr_polarity_score'] = np.vectorize(polarity_score)(df['qqdmr_positive_score'],df['qqdmr_negative_score'])\n",
    "df['qqdmr_average_sentence_length'] = df.qqd_extract.apply(average_sentence_length)\n",
    "df['qqdmr_percentage_of_complex_words'] = df.qqd_extract.apply(percentage_complex_word)\n",
    "df['qqdmr_fog_index'] = np.vectorize(fog_index)(df['qqdmr_average_sentence_length'],df['qqdmr_percentage_of_complex_words'])\n",
    "df['qqdmr_complex_word_count']= df.qqd_extract.apply(complex_word_count)\n",
    "df['qqdmr_word_count'] = df.qqd_extract.apply(total_word_count)\n",
    "df['qqdmr_uncertainty_score']=df.qqd_extract.apply(uncertainty_score)\n",
    "df['qqdmr_constraining_score'] = df.qqd_extract.apply(constraining_score)\n",
    "df['qqdmr_positive_word_proportion'] = np.vectorize(positive_word_prop)(df['qqdmr_positive_score'],df['qqdmr_word_count'])\n",
    "df['qqdmr_negative_word_proportion'] = np.vectorize(negative_word_prop)(df['qqdmr_negative_score'],df['qqdmr_word_count'])\n",
    "df['qqdmr_uncertainty_word_proportion'] = np.vectorize(uncertain_word_prop)(df['qqdmr_uncertainty_score'],df['qqdmr_word_count'])\n",
    "df['qqdmr_constraining_word_proportion'] = np.vectorize(constraining_word_prop)(df['qqdmr_constraining_score'],df['qqdmr_word_count'])\n",
    "\n",
    "df['rf_positive_score'] = df.riskfactor_extract.apply(positive_score)\n",
    "df['rf_negative_score'] = df.riskfactor_extract.apply(negative_word)\n",
    "df['rf_polarity_score'] = np.vectorize(polarity_score)(df['rf_positive_score'],df['rf_negative_score'])\n",
    "df['rf_average_sentence_length'] = df.riskfactor_extract.apply(average_sentence_length)\n",
    "df['rf_percentage_of_complex_words'] = df.riskfactor_extract.apply(percentage_complex_word)\n",
    "df['rf_fog_index'] = np.vectorize(fog_index)(df['rf_average_sentence_length'],df['rf_percentage_of_complex_words'])\n",
    "df['rf_complex_word_count']= df.riskfactor_extract.apply(complex_word_count)\n",
    "df['rf_word_count'] = df.riskfactor_extract.apply(total_word_count)\n",
    "df['rf_uncertainty_score']=df.riskfactor_extract.apply(uncertainty_score)\n",
    "df['rf_constraining_score'] = df.riskfactor_extract.apply(constraining_score)\n",
    "df['rf_positive_word_proportion'] = np.vectorize(positive_word_prop)(df['rf_positive_score'],df['rf_word_count'])\n",
    "df['rf_negative_word_proportion'] = np.vectorize(negative_word_prop)(df['rf_negative_score'],df['rf_word_count'])\n",
    "df['rf_uncertainty_word_proportion'] = np.vectorize(uncertain_word_prop)(df['rf_uncertainty_score'],df['rf_word_count'])\n",
    "df['rf_constraining_word_proportion'] = np.vectorize(constraining_word_prop)(df['rf_constraining_score'],df['rf_word_count'])\n",
    "\n",
    "df['constraining_words_whole_report'] = np.vectorize(constrain_word_whole)(df['mda_extract'],df['qqd_extract'],df['riskfactor_extract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd3e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b70dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f683604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57610c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db361e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b316e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
